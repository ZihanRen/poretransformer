Project: config_transformer
experiment: ex7
architecture:
  block_size: 216
  vocab_size: 3001
  n_layer: 12
  n_head: 8
  n_embd: 512
  dropout: 0.01
  bias: true
  cond_dim: 4
  cond_embd: 50
  tokens_embd: 256
train:
  p_keep: 0.98
  sos_token: 3000
  load_model: false
  pretrained_transformer_epoch: -2
  pretrained_vqgan_epoch: 10
  batch_size: 50
  epochs: 500
  learning_rate: 0.0005
  betas:
  - 0.99
  - 0.999
  weight_decay: 0.01
