Project: config_transformer
experiment: ex6
architecture:
  lock_size: 512
  vocab_size: 3000
  n_layer: 12
  n_head: 12
  n_embd: 1080
  dropout: 0.0
  bias: true
data:
  PATH:
    main_vol: /journel/s0/zur74/data/new_energy_well/train_vol/main_vol
    sub_vol: /journel/s0/zur74/data/new_energy_well/train_vol/sub_vol
  ct_idx:
  - 2
  - 3
  - 4
  - 5
checkpoints:
  PATH: /journel/s0/zur74/LatentPoreUpscale3DNet/lpu3dnet/train/checkpoints
train:
  epochs: 300
  batch_size: 20
  lr_vqgan: 0.0005
  lr_disc: 0.0001
  beta1: 0.9
  beta2: 0.999
  disc_factor: 0.2
  disc_start: 1000
  w_embed: 0.02
  codebook_weight_increase_per_epoch: 0.02
  drop_last: true
  g_lambda: 10
  max_weight_q_loss: 2
  load_model: false
  pretrained_model_epoch: None
