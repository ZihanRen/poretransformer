Project: config_vqgan

architecture:
  encoder:
    img_channels: 1
    latent_dim: 256
    num_res_blocks: 2
    num_groups: 16
    # reduce from 64->2: 64/(2^5) = 2
    # if you want to have a codebook size of 4^3, reduce to 256  
    channels: [16,16,64,128,256,512]
  
  codebook:
    # codebook size
    size: 10000
    # codebook dimension
    latent_dim: 256
    # weight of commitment loss for z_n (embedding)
    beta_c: 0.2
    autoencoder: false # whether to use autoencoder to replace genereator boolean

  decoder:
    img_channels: 1
    latent_dim: 256
    num_res_blocks: 3
    num_groups: 16
    # 2->4->8->16->32->64 (no upsampling for last layer & first layer)
    channels: [512, 256, 128, 128, 64, 16, 16]

  discriminator:
    img_channels: 1
    init_filters_num: 64
    num_layers: 3

# path of data and checkpoints
data:
  PATH:
    main_vol: /journel/s0/zur74/data/new_energy_well/train_vol/main_vol
    sub_vol: /journel/s0/zur74/data/new_energy_well/train_vol/sub_vol
  ct_idx: [2,3,4,5]

checkpoints:
  PATH: /journel/s0/zur74/LatentPoreUpscale3DNet/lpu3dnet/train/checkpoints



# training parameters
train:
  epochs: 100
  batch_size: 30
  lr_vqgan: 0.0005 #TODO check - learning rate for vqgan
  lr_disc: 0.0001 # learning rate for discriminator
  beta1: 0.9
  beta2: 0.999
  disc_factor: 0.2 # weight of discriminator loss
  disc_start: 1000 # steps before starting discriminator
  # consider making w_embed dynamic adjusted by steps
  w_embed: 0.1 # weight of embedding loss weight*q_loss + (1-weight)*rec_loss; >0.5 -- larger weight on q loss

