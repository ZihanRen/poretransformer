Project: config_vqgan

experiment: ex11

usecodebook_ema: false  # Set to true to use Codebook_EMA
pretrained: false
usecodebook_topk: false # Set to true to use Codebook_topk module


architecture:
  encoder:
    img_channels: 1
    latent_dim: 256
    num_res_blocks: 2
    num_groups: 16
    # reduce from 64->2: 64/(2^5) = 2
    # if you want to have a codebook size of 4^3, reduce to 256 or reduce one more layer after 16  
    channels: [16,64,128,256,512]
    decrease_features: true
  
  codebook:
    # codebook size
    size: 3000
    # codebook dimension
    latent_dim: 256
    # weight of commitment loss for z_n (embedding)
    beta_c: 1
    autoencoder: false # whether to use autoencoder to replace genereator boolean
    legacy: false # whether to use legacy codebook boolean

  codebookEMA:
    # codebook size
    size: 3000
    # codebook dimension
    latent_dim: 256
    # weight of commitment loss for z_n (embedding)
    beta_c: 1 
    decay: 0.9 # whether to use autoencoder to replace genereator boolean


  decoder:
    img_channels: 1
    latent_dim: 256
    num_res_blocks: 3
    num_groups: 16
    # 2->4->8->16->32->64 (no upsampling for last layer & first layer)
    channels: [512, 256, 256, 64, 16, 16]
    decrease_features: true

  discriminator:
    img_channels: 1
    init_filters_num: 64
    num_layers: 3


# training parameters
train:
  epochs: 100
  batch_size: 20
  lr_vqgan: 0.0003 #TODO check - learning rate for vqgan
  lr_disc: None # learning rate for discriminator
  beta1: 0.9
  beta2: 0.999
  disc_factor: None # weight of discriminator loss
  disc_start: None # steps before starting discriminator
  # consider making w_embed dynamic adjusted by steps
  w_embed: None # weight of embedding loss weight*q_loss + (1-weight)*rec_loss; >0.5 -- larger weight on q loss
  codebook_weight_increase_per_epoch: 0.2 # increase codebook weight by this amount per epoch until reaching to 1
  drop_last: False # whether to drop last batch if it is smaller than batch_size
  g_lambda: None # weight of gradient penalty
  max_weight_q_loss: 1.2 # maximum weight of q loss
  load_model: false
  pretrained_model_epoch: None
  l2_reg_weight: None
