Project: config_transformer
experiment: ex10


architecture:
  block_size: 216 # 3^3 * 8: 8 patches of 27 tokens so total is 27*8
  vocab_size: 3001  # codebook size of VQ-VAE
  n_layer: 12
  n_head: 12 # number of heads in multiheadattention model
  n_embd: 1080 # attention features dimension
  dropout: 0.01 
  bias: True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
  cond_dim: 4 # number of conditional attributes - ijk and porosity + 1 noise dimension
  cond_embd: 300 # expand dimension of features - tune this parameter to control power of conditioning generation
  tokens_embd: 3000 # dimension of token embeddings initially
  features_num: 27



# training parameters
# TODO
train:
  p_keep: None #TODO
  sos_token: 3000 # idx should be larger than vocab_size (3000) - 1
  load_model: False
  pretrained_transformer_epoch: None
  pretrained_vqgan_epoch: None # identify which VQGAN to use for sampling
  batch_size: 50 #TODO
  epochs: 500
  learning_rate: 0.0002 #TODO check - learning rate for vqgan
  betas: [0.99,0.999] # learning rate for discriminator
  weight_decay: 0.01 #TODO check

  
