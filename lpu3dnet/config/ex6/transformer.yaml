Project: config_transformer
experiment: ex6


architecture:
  block_size: 512 # 3^3 * 8: 8 patches of 27 tokens so total is 27*8
  vocab_size: 3001  # codebook size of VQ-VAE
  n_layer: 12
  n_head: 8 # number of heads in multiheadattention model
  n_embd: 512 # attention features dimension
  dropout: 0.01 
  bias: True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
  cond_dim: 4 # number of conditional attributes - ijk and porosity
  cond_embd: 300 # expand dimension of features - tune this parameter to control power of conditioning generation
  tokens_embd: 3000 # dimension of token embeddings initially
  features_num: 64


# path of data and checkpoints
data:
  PATH:
    main_vol: /journel/s0/zur74/data/new_energy_well/train_vol/main_vol
    sub_vol: /journel/s0/zur74/data/new_energy_well/train_vol/sub_vol
  ct_idx: [2,3,4,5]

checkpoints:
  PATH: /journel/s0/zur74/LatentPoreUpscale3DNet/lpu3dnet/train/checkpoints


# training parameters
# TODO
train:
  p_keep: None #TODO
  sos_token: 3000 # idx should be larger than vocab_size (3000) - 1
  load_model: False
  pretrained_transformer_epoch: None
  pretrained_vqgan_epoch: None # identify which VQGAN to use for sampling
  batch_size: 50 #TODO
  epochs: 500
  learning_rate: 0.0003 #TODO check - learning rate for vqgan
  betas: [0.99,0.999] # learning rate for discriminator
  weight_decay: 0.01 #TODO check

  
