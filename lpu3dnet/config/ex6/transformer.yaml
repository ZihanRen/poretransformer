Project: config_transformer
experiment: ex6


architecture:
  block_size: 512 # 1728 for 3D, 512 for 2D # TODO Investigate lock size
  vocab_size: 3000  # codebook size of VQ-VAE
  n_layer: 12 
  n_head: 12 # number of heads in multiheadattention model
  n_embd: 1080 # TODO investigate
  dropout: 0.0 # TODO investigate
  bias: True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster


# path of data and checkpoints
data:
  PATH:
    main_vol: /journel/s0/zur74/data/new_energy_well/train_vol/main_vol
    sub_vol: /journel/s0/zur74/data/new_energy_well/train_vol/sub_vol
  ct_idx: [2,3,4,5]

checkpoints:
  PATH: /journel/s0/zur74/LatentPoreUpscale3DNet/lpu3dnet/train/checkpoints


# training parameters
# TODO
train:
  p_keep: 0.9
  sos_token: 0
  load_model: False
  pretrained_transformer_epoch: -1
  pretrained_vqgan_epoch: 10 # identify which VQGAN to use for sampling
  batch_size: 20 #TODO
  epochs: 10
  learning_rate: 0.0005 #TODO check - learning rate for vqgan
  betas: [0.99,0.999] # learning rate for discriminator
  weight_decay: 0.8

  
