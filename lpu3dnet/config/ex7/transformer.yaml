Project: config_transformer
experiment: ex7


architecture:
  block_size: 216 # 3^3 * 8: 8 patches of 27 tokens so total is 27*8
  vocab_size: 3001  # codebook size of VQ-VAE
  n_layer: 12 
  n_head: 8 # number of heads in multiheadattention model
  n_embd: 512 # attention features dimension
  dropout: 0.01 
  bias: True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
  cond_dim: 4 # number of conditional attributes - ijk and porosity
  cond_embd: 50 # expand dimension of features - tune this parameter to control power of conditioning generation
  tokens_embd: 256 # dimension of token embeddings initially



# training parameters
# TODO
train:
  p_keep: 0.98 #TODO
  sos_token: 3000 # idx should be larger than vocab_size (3000) - 1
  load_model: False
  pretrained_transformer_epoch: -2
  pretrained_vqgan_epoch: 10 # identify which VQGAN to use for sampling
  batch_size: 50 #TODO
  epochs: 500
  learning_rate: 0.0005 #TODO check - learning rate for vqgan
  betas: [0.99,0.999] # learning rate for discriminator
  weight_decay: 0.01 #TODO check

  
