Project: config_transformer
experiment: ex7


architecture:
  block_size: 64 # 1728 for 3D, 512 for 2D # TODO Investigate lock size
  vocab_size: 3001  # codebook size of VQ-VAE
  n_layer: 12 
  n_head: 12 # number of heads in multiheadattention model
  n_embd: 1080 # TODO investigate
  dropout: 0.2 # TODO investigate
  bias: True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster


# path of data and checkpoints
data:
  PATH:
    main_vol: /journel/s0/zur74/data/new_energy_well/train_vol/main_vol
    sub_vol: /journel/s0/zur74/data/new_energy_well/train_vol/sub_vol
  ct_idx: [2,3,4,5]

checkpoints:
  PATH: /journel/s0/zur74/LatentPoreUpscale3DNet/lpu3dnet/train/checkpoints


# training parameters
# TODO
train:
  p_keep: 0.9 #TODO
  sos_token: 3000 # idx should be larger than vocab_size (3000) - 1
  load_model: False
  pretrained_transformer_epoch: -2
  pretrained_vqgan_epoch: 10 # identify which VQGAN to use for sampling
  batch_size: 50 #TODO
  epochs: 60
  learning_rate: 0.0003 #TODO check - learning rate for vqgan
  betas: [0.99,0.999] # learning rate for discriminator
  weight_decay: 0.01 #TODO check

  
