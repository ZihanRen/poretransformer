Project: config_vqgan

experiment: ex7

usecodebook_ema: false  # Set to true to use Codebook_EMA
pretrained: false
usecodebook_topk: false # Set to true to use Codebook_topk module


architecture:
  encoder:
    img_channels: 1
    latent_dim: 256
    num_res_blocks: 2
    num_groups: 16
    # reduce from 64->2: 64/(2^5) = 2
    # if you want to have a codebook size of 4^3, reduce to 256 or reduce one more layer after 16  
    channels: [16,64,128,256,512]
    decrease_features: true
  
  codebook:
    # codebook size
    size: 3000
    # codebook dimension
    latent_dim: 256
    # weight of commitment loss for z_n (embedding)
    beta_c: 1
    autoencoder: false # whether to use autoencoder to replace genereator boolean
    legacy: false # whether to use legacy codebook boolean

  codebookEMA:
    # codebook size
    size: 3000
    # codebook dimension
    latent_dim: 256
    # weight of commitment loss for z_n (embedding)
    beta_c: 1 
    decay: 0.9 # whether to use autoencoder to replace genereator boolean


  decoder:
    img_channels: 1
    latent_dim: 256
    num_res_blocks: 3
    num_groups: 16
    # 2->4->8->16->32->64 (no upsampling for last layer & first layer)
    channels: [512, 256, 256, 64, 16, 16]
    decrease_features: true

  discriminator:
    img_channels: 1
    init_filters_num: 64
    num_layers: 3

# path of data and checkpoints
data:
  PATH:
    main_vol: /journel/s0/zur74/data/new_energy_well/train_vol/main_vol
    sub_vol: /journel/s0/zur74/data/new_energy_well/train_vol/sub_vol
  ct_idx: [2,3,4,5]

checkpoints:
  PATH: /journel/s0/zur74/LatentPoreUpscale3DNet/lpu3dnet/train/checkpoints


# training parameters
train:
  epochs: 300
  batch_size: 20
  lr_vqgan: 0.0005 #TODO check - learning rate for vqgan
  lr_disc: 0.0001 # learning rate for discriminator
  beta1: 0.9
  beta2: 0.999
  disc_factor: 0.2 # weight of discriminator loss
  disc_start: 1000 # steps before starting discriminator
  # consider making w_embed dynamic adjusted by steps
  w_embed: 0.02 # weight of embedding loss weight*q_loss + (1-weight)*rec_loss; >0.5 -- larger weight on q loss
  codebook_weight_increase_per_epoch: 0.02 # increase codebook weight by this amount per epoch until reaching to 1
  drop_last: true # whether to drop last batch if it is smaller than batch_size
  g_lambda: 10 # weight of gradient penalty
  max_weight_q_loss: 2 # maximum weight of q loss
  load_model: false
  pretrained_model_epoch: None
